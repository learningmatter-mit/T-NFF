{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import networkx as nx\n",
    "\n",
    "# sys.path.insert(0, '/path/to/temp_graph/')\n",
    "sys.path.insert(0, '/home/jurgis/PAPER/Temperature-transferable-NFF')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nff.train import Trainer, get_trainer, get_model, loss, hooks, metrics, evaluate, load_model\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import nff.data as d\n",
    "import pickle\n",
    "\n",
    "from nff.data import Dataset, split_train_validation_test, collate_dicts, sparsify_tensor\n",
    "from nff.io.ase import * \n",
    "from nff.nn.glue import Stack\n",
    "\n",
    "from ase import Atoms\n",
    "from ase.neighborlist import neighbor_list\n",
    "from nff.data.sparse import sparsify_array\n",
    "\n",
    "from nff.md.nvt import * \n",
    "from ase import units\n",
    "from nff.io import NeuralFF\n",
    "from nff.md.nve import * \n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "    ## There are more file names, because when training on the full dataset with \n",
    "    ## multiple temperatures, all of them are loaded together\n",
    "    PATH = './data'\n",
    "    CG_PATH1 = '{}/p300_CG_T300_intra7_inter8.pkl'.format(PATH)\n",
    "#     CG_PATH2 = '{}/p300_CG{}_T350_intra7_inter8.pkl'.format(PATH, CG_key)\n",
    "#     CG_PATH3 = '{}/p300_CG{}_T450_intra7_inter8.pkl'.format(PATH, CG_key)\n",
    "#     CG_PATH4 = '{}/p300_CG{}_T500_intra7_inter8.pkl'.format(PATH, CG_key)\n",
    "\n",
    "    props = pickle.load( open( CG_PATH1, \"rb\" ) )\n",
    "#     props2 = pickle.load( open( CG_PATH2, \"rb\" ) )\n",
    "#     props3 = pickle.load( open( CG_PATH3, \"rb\" ) )\n",
    "#     props4 = pickle.load( open( CG_PATH4, \"rb\" ) )\n",
    "\n",
    "    props['cell'] = [torch.Tensor(props['cell'][i]) for i, cell in enumerate(props['cell'])]\n",
    "#     props2['cell'] = [torch.Tensor(props2['cell'][i]) for i, cell in enumerate(props2['cell'])]\n",
    "#     props3['cell'] = [torch.Tensor(props3['cell'][i]) for i, cell in enumerate(props3['cell'])]\n",
    "#     props4['cell'] = [torch.Tensor(props4['cell'][i]) for i, cell in enumerate(props4['cell'])]\n",
    "\n",
    "#     for key in props:\n",
    "#         props[key] = props[key] + props2[key] + props3[key] + props4[key]\n",
    "\n",
    "    bond_dic = {'CCCC': [[5 * i, 5 * i + 1,\n",
    "                          5 * i + 1, 5 * i + 2,\n",
    "                          5 * i + 2, 5 * i + 3] for i in range(300)]}\n",
    "    bond_dic['CCCC'] = torch.LongTensor( np.array(bond_dic['CCCC']).reshape(900,2).tolist())\n",
    "\n",
    "#     bond_dic = {'CCCC': [[3 * i, 3 * i + 1] for i in range(300)]}\n",
    "#     bond_dic['CCCC'] = torch.LongTensor( np.array(bond_dic['CCCC']).reshape(300,2).tolist())\n",
    "    \n",
    "    # xyz = torch.Tensor([props['nxyz'][i][:,1:4] for i in range(len(props['nxyz']))])\n",
    "\n",
    "    # r = (xyz[:, bond_dic['CCCC'][:,0]] - xyz[:, bond_dic['CCCC'][:,1]]).pow(2).sum(-1).sqrt()\n",
    "\n",
    "    # print(r.reshape(-1, 300, 3)[:,:,0].mean())\n",
    "    # print(r.reshape(-1, 300, 3)[:,:,1].mean())\n",
    "    # print(r.reshape(-1, 300, 3)[:,:,2].mean())\n",
    "\n",
    "    props['bonds'] = [bond_dic['CCCC'] for i in range(len(props['nxyz']))]\n",
    "    props['num_bonds'] = [torch.LongTensor([3]) for i in range(len(props['nxyz']))]\n",
    "    props['bond_len'] = [torch.Tensor([2.2439, 2.8182, 2.5558]) for i in range(len(props['nxyz']))]\n",
    "\n",
    "    props['smiles'] = ['CCCC'] * len(props['nxyz'])\n",
    "    \n",
    "    temp = 1/np.array([300,350,450,500])\n",
    "    props['temp'] = torch.cat([torch.zeros(99)+temp[0]])#,torch.zeros(5997)+temp[1],\n",
    "#                               torch.zeros(5997)+temp[2],torch.zeros(5997)+temp[3]])\n",
    "\n",
    "    dataset = d.Dataset(props.copy(), units='kcal/mol')\n",
    "\n",
    "    train, val, test = split_train_validation_test(dataset, val_size=0.1, test_size=0.01)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=1, collate_fn=collate_dicts)\n",
    "    val_loader = DataLoader(val, batch_size=1, collate_fn=collate_dicts)\n",
    "    test_loader = DataLoader(test, batch_size=1, collate_fn=collate_dicts)\n",
    "    \n",
    "    return dataset, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_loader, val_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nff.utils import batch_to\n",
    "batch = batch_to( next(iter(train_loader)), \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default model, without any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model described in the paper\n",
    "modelparams = dict()\n",
    "modelparams['n_atom_basis'] = 160\n",
    "modelparams['n_filters'] = 256\n",
    "modelparams['n_gaussians'] = 64\n",
    "modelparams['mol_n_convolutions'] = 3\n",
    "modelparams['sys_n_convolutions'] = 3\n",
    "modelparams['mol_cutoff'] = 7\n",
    "modelparams['sys_cutoff'] = 8\n",
    "modelparams[\"V_ex_power\"] = 10\n",
    "modelparams[\"V_ex_sigma\"] = 3.8553023965125024\n",
    "modelparams['dropout_rate'] = 0\n",
    "modelparams['temp_type'] = 'mult'\n",
    "\n",
    "bondparams = dict()\n",
    "bondparams['k'] = 20.758080279097705\n",
    "bondparams['dif_bond_len'] = True\n",
    "\n",
    "bondprior = get_model(bondparams, model_type='BondPrior')\n",
    "temp_transfer = get_model(modelparams, model_type='cg_temp_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy': tensor([[3161.5049]], grad_fn=<AsStridedBackward>),\n",
       " 'energy_grad': tensor([[  0.2339,  -0.5899,   1.5959],\n",
       "         [ -2.0033,   0.6025,  -3.8252],\n",
       "         [-11.3451,  22.1112,   7.0605],\n",
       "         ...,\n",
       "         [  6.2783,  -3.3613,   2.4024],\n",
       "         [  0.7824,   1.4860,  -0.0779],\n",
       "         [  1.8330,  -0.6764,   4.4105]], grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nff.nn.glue import Stack\n",
    "model_dict = dict()\n",
    "model_dict['bondprior'] = bondprior\n",
    "model_dict['temp_transfer'] = temp_transfer\n",
    "stack = Stack(model_dict, mode='sum')\n",
    "stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './example/training/dummy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists\n"
     ]
    }
   ],
   "source": [
    "loss_fn = loss.build_mse_loss(loss_coef={'energy_grad': 1})\n",
    "\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, stack.parameters()) # CHANGE PARAMTERS\n",
    "optimizer = Adam(trainable_params, lr=3e-4)\n",
    "\n",
    "\n",
    "train_metrics = [\n",
    "    metrics.MeanAbsoluteError('energy_grad')\n",
    "]\n",
    "\n",
    "from shutil import rmtree\n",
    "import os\n",
    "\n",
    "train_hooks = [\n",
    "    hooks.MaxEpochHook(100),\n",
    "    hooks.CSVHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "    ),\n",
    "    hooks.PrintingHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "        separator = ' | ',\n",
    "        time_strf='%M:%S'\n",
    "    ),\n",
    "    hooks.ReduceLROnPlateauHook(\n",
    "        optimizer=optimizer,\n",
    "        patience=30,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-7,\n",
    "        window_length=1,\n",
    "        stop_after_min=True\n",
    "    )\n",
    "]\n",
    "\n",
    "if os.path.exists(OUTDIR):\n",
    "    print('exists')\n",
    "    rmtree(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Trainer(\n",
    "    model_path=OUTDIR,\n",
    "    model=stack,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    checkpoint_interval=1,\n",
    "    hooks=train_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time | Epoch | Learning rate | Train loss | Validation loss | MAE_energy_grad | GPU Memory (MB)\n",
      "25:54 |     1 |     3.000e-04 |   309.8748 |        298.8344 |         12.3961 |             522\n"
     ]
    }
   ],
   "source": [
    "T.train(device=0, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The models used in the paper are reported in ./models repo.\n",
    "The temperature transferable embedding model is located in the t-nff directory, and the non temperature-embedding in the nff one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './models/t_nff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.neighborlist import neighbor_list\n",
    "from nff.data.sparse import sparsify_array\n",
    "\n",
    "from nff.md.nvt import * \n",
    "from ase import units\n",
    "from nff.io import NeuralFF\n",
    "from nff.md.nve import * \n",
    "\n",
    "\n",
    "DEFAULT_CUTOFF = 5.0\n",
    "\n",
    "system_prop = {key: val[0] for key, val in dataset.props.items()}\n",
    "system_prop['atoms_cutoff'] = 7\n",
    "system_prop['system_cutoff'] = 8\n",
    "system_prop['temp'] = torch.zeros(1)+1/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nff.io.ase import BulkPhaseMaterials\n",
    "\n",
    "bulk = BulkPhaseMaterials(numbers=[1, 2, 3, 4, 5] * 300, \n",
    "                          positions=dataset.props['nxyz'][0][:, 1:4],\n",
    "                          cell=dataset.props['cell'][0],\n",
    "                          pbc=True,\n",
    "                          props=system_prop)\n",
    "bulk.set_masses( [29.0407, 53.0607, 28.052, 29.06, 86.804612] * 300) # mass of cg atoms  \n",
    "bulk.update_nbr_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time[ps]      Etot[eV]     Epot[eV]     Ekin[eV]    T[K]\n",
      "0.0000           153.51        92.00        61.51   317.2\n",
      "\n",
      "0.1000           119.29        65.89        53.41   275.5\n",
      "\n",
      "0.2000           105.91        51.37        54.54   281.3\n",
      "\n",
      "0.3000           108.45        46.59        61.86   319.1\n",
      "\n",
      "0.4000           108.63        46.00        62.63   323.0\n",
      "\n",
      "0.5000            97.73        42.52        55.21   284.7\n",
      "\n",
      "0.6000            95.32        40.07        55.25   284.9\n",
      "\n",
      "0.7000            98.35        37.83        60.52   312.2\n",
      "\n",
      "0.8000           101.52        38.78        62.75   323.6\n",
      "\n",
      "0.9000            91.97        36.13        55.84   288.0\n",
      "\n",
      "1.0000            91.23        36.74        54.49   281.0\n",
      "\n",
      "1.1000            94.46        35.69        58.76   303.1\n",
      "\n",
      "1.2000            97.60        35.55        62.05   320.0\n",
      "\n",
      "1.3000            91.03        34.07        56.96   293.8\n",
      "\n",
      "1.4000            88.79        33.75        55.03   283.8\n",
      "\n",
      "1.5000            92.47        31.37        61.09   315.1\n",
      "\n",
      "1.6000            90.10        29.53        60.57   312.4\n",
      "\n",
      "1.7000            88.25        30.83        57.42   296.1\n",
      "\n",
      "1.8000            85.14        31.12        54.01   278.6\n",
      "\n",
      "1.9000            93.17        35.01        58.16   300.0\n",
      "\n",
      "2.0000            93.97        33.52        60.45   311.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestep = 1\n",
    "steps = 2000\n",
    "temperature = 300\n",
    "\n",
    "path = '{}/ase_T300'.format(OUTDIR)\n",
    "if os.path.exists(path):\n",
    "    print('exists')\n",
    "else:\n",
    "    os.makedirs(path)\n",
    "\n",
    "DEFAULTNVEPARAMS = {\n",
    "    'T_init': temperature, \n",
    "    'thermostat': NoseHoover,   \n",
    "    'thermostat_params': {'timestep': timestep * units.fs, \"temperature\": temperature * units.kB,  \"ttime\": 20.0},\n",
    "    'nbr_list_update_freq': 10,\n",
    "    'steps': steps/timestep,\n",
    "    'save_frequency': 100/timestep,\n",
    "    'thermo_filename': '{}/thermo.log'.format(path, temperature), \n",
    "    'traj_filename': '{}/atoms.traj'.format(path, temperature),\n",
    "    'skip': 0\n",
    "}\n",
    "\n",
    "\n",
    "calc = NeuralFF(model=model, device=0)\n",
    "bulk.set_calculator(calc)\n",
    "nve = Dynamics(bulk, DEFAULTNVEPARAMS)\n",
    "\n",
    "nve.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:htvs] *",
   "language": "python",
   "name": "conda-env-htvs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
