{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import networkx as nx\n",
    "\n",
    "# sys.path.insert(0, '/path/to/temp_graph/')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nff.train import Trainer, get_trainer, get_model, loss, hooks, metrics, evaluate, load_model\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import nff.data as d\n",
    "import pickle\n",
    "\n",
    "from nff.data import Dataset, split_train_validation_test, collate_dicts, sparsify_tensor\n",
    "from nff.io.ase import * \n",
    "from nff.nn.glue import Stack\n",
    "\n",
    "from ase import Atoms\n",
    "from ase.neighborlist import neighbor_list\n",
    "from nff.data.sparse import sparsify_array\n",
    "\n",
    "from nff.md.nvt import * \n",
    "from ase import units\n",
    "from nff.io import NeuralFF\n",
    "from nff.md.nve import * \n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "    ## There are more file names, because when training on the full dataset with \n",
    "    ## multiple temperatures, all of them are loaded together\n",
    "    PATH = './data'\n",
    "    CG_PATH1 = '{}/p300_CG5_T300_intra7_inter8.pkl'.format(PATH)\n",
    "    CG_PATH2 = '{}/p300_CG5_T350_intra7_inter8.pkl'.format(PATH)\n",
    "    CG_PATH3 = '{}/p300_CG5_T450_intra7_inter8.pkl'.format(PATH)\n",
    "    CG_PATH4 = '{}/p300_CG5_T500_intra7_inter8.pkl'.format(PATH)\n",
    "\n",
    "    props = pickle.load( open( CG_PATH1, \"rb\" ) )\n",
    "    props2 = pickle.load( open( CG_PATH2, \"rb\" ) )\n",
    "    props3 = pickle.load( open( CG_PATH3, \"rb\" ) )\n",
    "    props4 = pickle.load( open( CG_PATH4, \"rb\" ) )\n",
    "\n",
    "    props['cell'] = [torch.Tensor(props['cell'][i]) for i, cell in enumerate(props['cell'])]\n",
    "    props2['cell'] = [torch.Tensor(props2['cell'][i]) for i, cell in enumerate(props2['cell'])]\n",
    "    props3['cell'] = [torch.Tensor(props3['cell'][i]) for i, cell in enumerate(props3['cell'])]\n",
    "    props4['cell'] = [torch.Tensor(props4['cell'][i]) for i, cell in enumerate(props4['cell'])]\n",
    "\n",
    "    for key in props:\n",
    "        props[key] = props[key] + props2[key] + props3[key] + props4[key]\n",
    "\n",
    "    bond_dic = {'CCCC': [[5 * i, 5 * i + 1,\n",
    "                          5 * i + 1, 5 * i + 2,\n",
    "                          5 * i + 2, 5 * i + 3] for i in range(300)]}\n",
    "    bond_dic['CCCC'] = torch.LongTensor( np.array(bond_dic['CCCC']).reshape(900,2).tolist())\n",
    "\n",
    "    props['bonds'] = [bond_dic['CCCC'] for i in range(len(props['nxyz']))]\n",
    "    props['num_bonds'] = [torch.LongTensor([3]) for i in range(len(props['nxyz']))]\n",
    "    props['bond_len'] = [torch.Tensor([2.2439, 2.8182, 2.5558]) for i in range(len(props['nxyz']))]\n",
    "\n",
    "    props['smiles'] = ['CCCC'] * len(props['nxyz'])\n",
    "    \n",
    "    temp = 1/np.array([300,350,450,500])\n",
    "    props['temp'] = torch.cat([torch.zeros(5997)+temp[0],torch.zeros(5997)+temp[1],\n",
    "                              torch.zeros(5997)+temp[2],torch.zeros(5997)+temp[3]])\n",
    "\n",
    "    dataset = d.Dataset(props.copy(), units='kcal/mol')\n",
    "\n",
    "    train, val, test = split_train_validation_test(dataset, val_size=0.1, test_size=0.01)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=1, collate_fn=collate_dicts)\n",
    "    val_loader = DataLoader(val, batch_size=1, collate_fn=collate_dicts)\n",
    "    test_loader = DataLoader(test, batch_size=1, collate_fn=collate_dicts)\n",
    "    \n",
    "    return dataset, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_loader, val_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nff.utils import batch_to\n",
    "batch = batch_to( next(iter(train_loader)), \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default model, without any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model described in the paper\n",
    "modelparams = dict()\n",
    "modelparams['n_atom_basis'] = 240\n",
    "modelparams['n_filters'] = 256\n",
    "modelparams['n_gaussians'] = 80\n",
    "modelparams['mol_n_convolutions'] = 3\n",
    "modelparams['sys_n_convolutions'] = 2\n",
    "modelparams['mol_cutoff'] = 7\n",
    "modelparams['sys_cutoff'] = 8\n",
    "modelparams[\"V_ex_power\"] = 7\n",
    "modelparams[\"V_ex_sigma\"] = 5.730579\n",
    "modelparams['dropout_rate'] = 0\n",
    "modelparams['temp_type'] = 'mult'\n",
    "\n",
    "bondparams = dict()\n",
    "bondparams['k'] = 11.286249\n",
    "bondparams['dif_bond_len'] = True\n",
    "\n",
    "bondprior = get_model(bondparams, model_type='BondPrior')\n",
    "temp_transfer = get_model(modelparams, model_type='cg_temp_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy': tensor([[31372.5039]], grad_fn=<AsStridedBackward>),\n",
       " 'energy_grad': tensor([[ -9.5633,  -7.0947,   0.6039],\n",
       "         [ -4.9302,  -7.5194,  23.1142],\n",
       "         [ -6.7983,   9.3507,   7.6828],\n",
       "         ...,\n",
       "         [ 14.8118,  10.5061,   4.0798],\n",
       "         [-20.4616, -12.3678,  -2.4416],\n",
       "         [ 10.1174,   0.5738,   7.3089]], grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nff.nn.glue import Stack\n",
    "model_dict = dict()\n",
    "model_dict['bondprior'] = bondprior\n",
    "model_dict['temp_transfer'] = temp_transfer\n",
    "stack = Stack(model_dict, mode='sum')\n",
    "stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './models/t_nff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = loss.build_mse_loss(loss_coef={'energy_grad': 1})\n",
    "\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, stack.parameters()) # CHANGE PARAMTERS\n",
    "optimizer = Adam(trainable_params, lr=3e-4)\n",
    "\n",
    "\n",
    "train_metrics = [\n",
    "    metrics.MeanAbsoluteError('energy_grad')\n",
    "]\n",
    "\n",
    "from shutil import rmtree\n",
    "import os\n",
    "\n",
    "train_hooks = [\n",
    "    hooks.MaxEpochHook(100),\n",
    "    hooks.CSVHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "    ),\n",
    "    hooks.PrintingHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "        separator = ' | ',\n",
    "        time_strf='%M:%S'\n",
    "    ),\n",
    "    hooks.ReduceLROnPlateauHook(\n",
    "        optimizer=optimizer,\n",
    "        patience=30,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-7,\n",
    "        window_length=1,\n",
    "        stop_after_min=True\n",
    "    )\n",
    "]\n",
    "\n",
    "if os.path.exists(OUTDIR):\n",
    "    print('exists')\n",
    "    rmtree(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Trainer(\n",
    "    model_path=OUTDIR,\n",
    "    model=stack,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    checkpoint_interval=1,\n",
    "    hooks=train_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time | Epoch | Learning rate | Train loss | Validation loss | MAE_energy_grad | GPU Memory (MB)\n",
      "11:56 |     1 |     3.000e-04 |   364.5730 |        355.2454 |         13.4167 |            1187\n",
      "32:33 |     2 |     3.000e-04 |   355.2182 |        353.7612 |         13.3803 |            1187\n"
     ]
    }
   ],
   "source": [
    "T.train(device=0, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The models used in the paper are reported in ./models repo.\n",
    "The temperature transferable embedding model is located in the t-nff directory, and the non temperature-embedding in the nff one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './models/t_nff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.neighborlist import neighbor_list\n",
    "from nff.data.sparse import sparsify_array\n",
    "\n",
    "from nff.md.nvt import * \n",
    "from ase import units\n",
    "from nff.io import NeuralFF\n",
    "from nff.md.nve import * \n",
    "\n",
    "\n",
    "DEFAULT_CUTOFF = 5.0\n",
    "\n",
    "system_prop = {key: val[0] for key, val in dataset.props.items()}\n",
    "system_prop['atoms_cutoff'] = 7\n",
    "system_prop['system_cutoff'] = 8\n",
    "system_prop['temp'] = torch.zeros(1)+1/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nff.io.ase import BulkPhaseMaterials\n",
    "\n",
    "bulk = BulkPhaseMaterials(numbers=[1, 2, 3, 4, 5] * 300, \n",
    "                          positions=dataset.props['nxyz'][0][:, 1:4],\n",
    "                          cell=dataset.props['cell'][0],\n",
    "                          pbc=True,\n",
    "                          props=system_prop)\n",
    "bulk.set_masses( [29.0407, 53.0607, 28.052, 29.06, 86.804612] * 300) # mass of cg atoms  \n",
    "bulk.update_nbr_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestep = 1\n",
    "steps = 10000\n",
    "temperature = 400\n",
    "\n",
    "path = '{}/ase_T400'.format(OUTDIR)\n",
    "if os.path.exists(path):\n",
    "    print('exists')\n",
    "else:\n",
    "    os.makedirs(path)\n",
    "\n",
    "DEFAULTNVEPARAMS = {\n",
    "    'T_init': temperature, \n",
    "    'thermostat': NoseHoover,   \n",
    "    'thermostat_params': {'timestep': timestep * units.fs, \"temperature\": temperature * units.kB,  \"ttime\": 20.0},\n",
    "    'nbr_list_update_freq': 10,\n",
    "    'steps': steps/timestep,\n",
    "    'save_frequency': 100/timestep,\n",
    "    'thermo_filename': '{}/thermo.log'.format(path, temperature), \n",
    "    'traj_filename': '{}/atoms.traj'.format(path, temperature),\n",
    "    'skip': 0\n",
    "}\n",
    "\n",
    "\n",
    "calc = NeuralFF(model=model, device=0)\n",
    "bulk.set_calculator(calc)\n",
    "nve = Dynamics(bulk, DEFAULTNVEPARAMS)\n",
    "\n",
    "nve.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:htvs] *",
   "language": "python",
   "name": "conda-env-htvs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
